# ğŸ” AUDITORÃA EP3 - OBSERVABILIDAD Y MONITOREO
**Proyecto:** Agente Inteligente PastelerÃ­a 1000 Sabores  
**Fecha:** 26 Noviembre 2025  
**EvaluaciÃ³n:** EP3 - ImplementaciÃ³n de Observabilidad

---

## 1. RESUMEN EJECUTIVO

### ğŸ“Š EvaluaciÃ³n General
Tu proyecto tiene una **base sÃ³lida de EP2** (agente con herramientas y memoria), pero **NO cumple con los requisitos de EP3** en su estado actual. Necesitas implementar componentes especÃ­ficos de observabilidad que actualmente estÃ¡n ausentes.

### âœ… Fortalezas
- Agente funcional con 4 herramientas
- Sistema de logging bÃ¡sico implementado
- ExecutionTracker capturando estadÃ­sticas
- Memoria dual operativa

### âŒ Debilidades CrÃ­ticas
- **Sin dashboard visual** (IE5 - 15% perdido)
- **Sin mÃ©tricas de precisiÃ³n/consistencia** (IE1 - 15% perdido)
- **Sin mÃ©tricas de recursos** (IE2 - parcial)
- **Sin anÃ¡lisis de patrones/anomalÃ­as** (IE4 - 10% perdido)
- **Sin protocolos de seguridad documentados** (IE6 - parcial)
- **Sin recomendaciones basadas en datos** (IE7 - 10% perdido)
- **Sin informe tÃ©cnico con evidencia visual** (IE8 - 5% perdido)

### ğŸ“ˆ Nota Proyectada Actual
**35-40/100 (2.5-3.0/7.0)** âš ï¸ INSUFICIENTE

### ğŸ¯ Nota Proyectada con Mejoras
**90-100/100 (6.5-7.0/7.0)** âœ… MUY BUEN DESEMPEÃ‘O

---

## 2. EVALUACIÃ“N POR INDICADOR

### **IE1: MÃ©tricas de PrecisiÃ³n, Consistencia y Errores (15%)**

ğŸ“Š **Nivel alcanzado:** âŒ **No logrado (0%)**

**Estado actual:**
- âœ… Tienes `ExecutionTracker` que cuenta ejecuciones
- âœ… Registras errores en logs
- âŒ **NO hay mÃ©trica de precisiÃ³n** (% respuestas correctas)
- âŒ **NO hay mÃ©trica de consistencia** (coherencia en respuestas similares)
- âŒ **NO hay cÃ¡lculo de frecuencia de errores** (errores/100 consultas)

**CÃ³digo actual:**
```python
# En logger.py lÃ­neas 190-223
def get_statistics(self):
    # Solo cuenta: total, successful, success_rate
    # FALTA: precisiÃ³n, consistencia, error_frequency
```

**Lo que necesitas:**

1. **Crear `src/monitoring/metrics.py`:**
```python
class ObservabilityMetrics:
    def __init__(self):
        self.total_queries = 0
        self.correct_responses = 0
        self.consistency_scores = []
        self.errors = []
    
    def calculate_precision(self):
        """PrecisiÃ³n = respuestas correctas / total"""
        if self.total_queries == 0:
            return 0
        return (self.correct_responses / self.total_queries) * 100
    
    def calculate_consistency(self, query, response):
        """Mide coherencia comparando con respuestas previas similares"""
        # Usar embeddings para comparar similitud
        pass
    
    def calculate_error_frequency(self):
        """Errores por cada 100 consultas"""
        if self.total_queries == 0:
            return 0
        return (len(self.errors) / self.total_queries) * 100
```

2. **Integrar en `app_agent.py`:**
```python
# DespuÃ©s de cada query, evaluar:
metrics.total_queries += 1
if user_validates_response():  # Thumbs up/down
    metrics.correct_responses += 1
```

**Impacto:** +15% si implementas correctamente

---

### **IE2: MÃ©tricas de Latencia y Uso de Recursos (15%)**

ğŸ“Š **Nivel alcanzado:** âš ï¸ **Aceptable (60% = 9/15 puntos)**

**Estado actual:**
- âœ… Mides tiempo de ejecuciÃ³n (`execution_time`)
- âŒ **NO mides uso de RAM**
- âŒ **NO mides uso de CPU**
- âŒ **NO mides tokens consumidos**
- âŒ **NO contextualizas** (promedio, mÃ¡x, mÃ­n)

**CÃ³digo actual:**
```python
# En agent_executor.py lÃ­nea 171
result["execution_time"] = time.time() - start_time
# SOLO tiempo, falta RAM, CPU, tokens
```

**Lo que necesitas:**

```python
import psutil
import tiktoken

class ResourceMetrics:
    def measure_resources(self, query, response):
        return {
            'latency_ms': execution_time * 1000,
            'memory_mb': psutil.Process().memory_info().rss / 1024 / 1024,
            'cpu_percent': psutil.cpu_percent(interval=0.1),
            'tokens_prompt': len(tiktoken.encode(query)),
            'tokens_response': len(tiktoken.encode(response)),
            'tokens_total': tokens_prompt + tokens_response
        }
```

**Impacto:** +6% (llegarÃ­as a 15/15)

---

### **IE3: AnÃ¡lisis de Logs y Trazabilidad (15%)**

ğŸ“Š **Nivel alcanzado:** âš ï¸ **Buen desempeÃ±o (80% = 12/15 puntos)**

**Estado actual:**
- âœ… Sistema de logging implementado (`logger.py`)
- âœ… Logs capturan query, tools, observations
- âœ… Logs almacenados en archivos
- âš ï¸ Logs en texto plano (no JSON estructurado)
- âŒ **NO hay anÃ¡lisis de logs** (identificaciÃ³n de errores/cuellos de botella)
- âŒ **NO documentas hallazgos** en informe

**Lo que necesitas:**

1. **Cambiar formato a JSON** (lÃ­nea 52 de logger.py):
```python
formatter = logging.Formatter(
    '{"timestamp": "%(asctime)s", "level": "%(levelname)s", "message": "%(message)s"}'
)
```

2. **Crear anÃ¡lisis de logs:**
```python
def analyze_logs(log_file):
    """Identifica errores y cuellos de botella"""
    errors = []
    slow_queries = []
    
    with open(log_file) as f:
        for line in f:
            log = json.loads(line)
            if log['level'] == 'ERROR':
                errors.append(log)
            if 'execution_time' in log and log['execution_time'] > 5:
                slow_queries.append(log)
    
    return {
        'total_errors': len(errors),
        'error_types': Counter([e['message'] for e in errors]),
        'bottlenecks': slow_queries
    }
```

**Impacto:** +3% (llegarÃ­as a 15/15)

---

### **IE4: IdentificaciÃ³n de Patrones y AnomalÃ­as (10%)**

ğŸ“Š **Nivel alcanzado:** âŒ **No logrado (0%)**

**Estado actual:**
- âŒ **NO hay anÃ¡lisis de patrones**
- âŒ **NO hay detecciÃ³n de anomalÃ­as**
- âŒ **NO hay propuestas de mejora basadas en datos**

**Lo que necesitas:**

```python
def identify_patterns(executions):
    """Analiza logs para encontrar patrones"""
    
    # PatrÃ³n 1: Errores por tipo de consulta
    error_by_query_type = {}
    
    # PatrÃ³n 2: Latencia por nÃºmero de herramientas
    latency_by_tools = {}
    
    # PatrÃ³n 3: Horarios pico
    queries_by_hour = {}
    
    # AnomalÃ­as
    anomalies = []
    avg_latency = mean([e['duration'] for e in executions])
    for e in executions:
        if e['duration'] > avg_latency * 3:  # 3x promedio
            anomalies.append({
                'type': 'high_latency',
                'query': e['query'],
                'latency': e['duration']
            })
    
    return {
        'patterns': {...},
        'anomalies': anomalies,
        'recommendations': [
            "Implementar cachÃ© para consultas frecuentes",
            "Optimizar herramientas lentas"
        ]
    }
```

**Documentar en informe:**
- "El 80% de errores ocurren con consultas de productos veganos"
- "Latencia aumenta 3x cuando se usan 3+ herramientas"
- "Picos de uso entre 12-14hrs"

**Impacto:** +10%

---

### **IE5: Dashboard Visual de Monitoreo (15%)**

ğŸ“Š **Nivel alcanzado:** âŒ **No logrado (0%)**

**Estado actual:**
- âŒ **NO existe dashboard dedicado**
- âš ï¸ Tienes mÃ©tricas bÃ¡sicas en sidebar de `app_agent.py`
- âŒ **NO hay visualizaciones** (grÃ¡ficos de lÃ­nea, barras)
- âŒ **NO es interactivo**

**Lo que necesitas:**

**Crear `dashboard.py`:**
```python
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
from src.monitoring.metrics import load_metrics

st.set_page_config(page_title="Dashboard Observabilidad", layout="wide")

st.title("ğŸ“Š Dashboard de Observabilidad - Agente IA")

# === PANEL 1: KPIs PRINCIPALES ===
col1, col2, col3, col4 = st.columns(4)
with col1:
    st.metric("Total Consultas", metrics['total_queries'])
with col2:
    st.metric("PrecisiÃ³n", f"{metrics['precision']:.1f}%")
with col3:
    st.metric("Latencia Promedio", f"{metrics['avg_latency']:.0f}ms")
with col4:
    st.metric("Tasa de Error", f"{metrics['error_rate']:.1f}%")

# === PANEL 2: GRÃFICO DE LATENCIA EN EL TIEMPO ===
st.subheader("â±ï¸ Latencia en el Tiempo")
df_latency = pd.DataFrame(metrics['latency_history'])
fig = px.line(df_latency, x='timestamp', y='latency_ms', 
              title='EvoluciÃ³n de Latencia')
st.plotly_chart(fig, use_container_width=True)

# === PANEL 3: USO DE RECURSOS ===
col1, col2 = st.columns(2)
with col1:
    st.subheader("ğŸ’¾ Uso de Memoria")
    fig = go.Figure(go.Indicator(
        mode="gauge+number",
        value=metrics['memory_mb'],
        title={'text': "RAM (MB)"},
        gauge={'axis': {'range': [None, 1000]}}
    ))
    st.plotly_chart(fig)

with col2:
    st.subheader("ğŸ”¥ Uso de CPU")
    fig = go.Figure(go.Indicator(
        mode="gauge+number",
        value=metrics['cpu_percent'],
        title={'text': "CPU (%)"},
        gauge={'axis': {'range': [None, 100]}}
    ))
    st.plotly_chart(fig)

# === PANEL 4: DISTRIBUCIÃ“N DE ERRORES ===
st.subheader("âŒ Tipos de Errores")
df_errors = pd.DataFrame(metrics['error_types'].items(), 
                         columns=['Tipo', 'Frecuencia'])
fig = px.bar(df_errors, x='Tipo', y='Frecuencia')
st.plotly_chart(fig, use_container_width=True)

# === PANEL 5: HERRAMIENTAS MÃS USADAS ===
st.subheader("ğŸ”§ Herramientas MÃ¡s Usadas")
df_tools = pd.DataFrame(metrics['tool_usage'].items(),
                        columns=['Herramienta', 'Usos'])
fig = px.pie(df_tools, values='Usos', names='Herramienta')
st.plotly_chart(fig)
```

**Ejecutar:**
```bash
streamlit run dashboard.py
```

**Capturar screenshots:**
1. Vista completa del dashboard
2. GrÃ¡fico de latencia
3. MÃ©tricas de recursos
4. DistribuciÃ³n de errores
5. Herramientas mÃ¡s usadas

**Impacto:** +15% (CRÃTICO)

---

### **IE6: Protocolos de Seguridad y Uso Responsable (10%)**

ğŸ“Š **Nivel alcanzado:** âš ï¸ **Aceptable (60% = 6/10 puntos)**

**Estado actual:**
- âœ… API key en `.env` (no en cÃ³digo)
- âŒ **NO hay validaciÃ³n de inputs**
- âŒ **NO hay rate limiting**
- âŒ **NO hay anonimizaciÃ³n en logs**
- âŒ **NO hay documentaciÃ³n de seguridad**

**Lo que necesitas:**

1. **ValidaciÃ³n de inputs** (`src/security/validator.py`):
```python
def validate_input(query: str) -> bool:
    if len(query) > 500:
        return False
    
    # Detectar inyecciones
    dangerous_patterns = [
        "ignore previous",
        "system:",
        "<script>",
        "DROP TABLE"
    ]
    for pattern in dangerous_patterns:
        if pattern.lower() in query.lower():
            return False
    return True
```

2. **Rate limiting** (`src/security/rate_limiter.py`):
```python
from datetime import datetime, timedelta

class RateLimiter:
    def __init__(self, max_requests=10, window_minutes=1):
        self.requests = {}
        self.max_requests = max_requests
        self.window = timedelta(minutes=window_minutes)
    
    def is_allowed(self, user_id):
        now = datetime.now()
        if user_id not in self.requests:
            self.requests[user_id] = []
        
        # Limpiar requests antiguos
        self.requests[user_id] = [
            t for t in self.requests[user_id]
            if now - t < self.window
        ]
        
        if len(self.requests[user_id]) >= self.max_requests:
            return False
        
        self.requests[user_id].append(now)
        return True
```

3. **Documentar en informe:**
```markdown
## Protocolos de Seguridad

### A) Seguridad
- âœ… API keys protegidas en .env
- âœ… ValidaciÃ³n de inputs (max 500 chars, sin inyecciones)
- âœ… Rate limiting (10 requests/minuto)

### B) Privacidad
- âœ… Logs anonimizan IDs de usuario
- âœ… No se guardan datos sensibles sin encriptar

### C) Ã‰tica
- âœ… Disclaimer sobre uso de IA
- âœ… Transparencia sobre limitaciones

### D) Cumplimiento
- âœ… TÃ©rminos de uso claros
- âœ… PolÃ­tica de retenciÃ³n de datos (30 dÃ­as)
```

**Impacto:** +4% (llegarÃ­as a 10/10)

---

### **IE7: Propuesta de Mejoras y OptimizaciÃ³n (10%)**

ğŸ“Š **Nivel alcanzado:** âŒ **No logrado (0%)**

**Estado actual:**
- âŒ **NO hay recomendaciones documentadas**
- âŒ **NO estÃ¡n basadas en datos observados**

**Lo que necesitas:**

En el informe, incluir secciÃ³n:

```markdown
## 5. RECOMENDACIONES DE MEJORA

### RecomendaciÃ³n 1: Implementar CachÃ© de Respuestas
**Problema detectado:**
- MÃ©trica: 40% de consultas son repetidas
- Dato: Latencia promedio 2.5s para queries idÃ©nticas

**Propuesta:**
Implementar cache con TTL de 1 hora para consultas frecuentes

**Impacto esperado:**
- Reducir latencia en 60% para queries cacheadas
- Ahorrar 30% en costos de tokens
- Mejorar experiencia de usuario

**Prioridad:** Alta  
**Esfuerzo:** Medio (4 horas)

---

### RecomendaciÃ³n 2: Optimizar Prompts
**Problema detectado:**
- MÃ©trica: Consumo promedio 1200 tokens/query
- Dato: Prompt system tiene 800 tokens

**Propuesta:**
Reducir prompt system de 800 a 400 tokens

**Impacto esperado:**
- Reducir costos en 25%
- Reducir latencia en 15%

**Prioridad:** Alta  
**Esfuerzo:** Bajo (2 horas)

---

### RecomendaciÃ³n 3: Paralelizar Herramientas
**Problema detectado:**
- MÃ©trica: Queries con 3 herramientas tardan 8s
- Dato: Herramientas son independientes

**Propuesta:**
Ejecutar herramientas en paralelo con asyncio

**Impacto esperado:**
- Reducir latencia en 50% para queries complejas

**Prioridad:** Media  
**Esfuerzo:** Alto (8 horas)
```

**Impacto:** +10%

---

### **IE8: Informe con Evidencia Visual (5%)**

ğŸ“Š **Nivel alcanzado:** âŒ **No logrado (0%)**

**Estado actual:**
- âŒ **NO existe informe tÃ©cnico**
- âŒ **NO hay capturas del dashboard**
- âŒ **NO hay grÃ¡ficos**

**Lo que necesitas:**

**Crear `docs/INFORME_EP3.pdf` (mÃ¡x 5 pÃ¡ginas):**

```markdown
# INFORME TÃ‰CNICO EP3
## ImplementaciÃ³n de Observabilidad - Agente IA

### 1. IntroducciÃ³n
[DescripciÃ³n del proyecto y objetivos EP3]

### 2. MÃ©tricas Implementadas

#### 2.1 MÃ©tricas de PrecisiÃ³n
[Captura del dashboard mostrando precisiÃ³n: 87.5%]
[GrÃ¡fico de evoluciÃ³n de precisiÃ³n en el tiempo]

#### 2.2 MÃ©tricas de Latencia
[Captura del grÃ¡fico de latencia]
- Promedio: 2.3s
- MÃ¡ximo: 8.1s
- MÃ­nimo: 0.8s

#### 2.3 MÃ©tricas de Recursos
[Captura de gauges de RAM y CPU]
- RAM promedio: 245 MB
- CPU promedio: 12%
- Tokens promedio: 1200/query

### 3. AnÃ¡lisis de Logs
[Captura de logs estructurados]
[Tabla con tipos de errores y frecuencia]

### 4. Patrones Identificados
[GrÃ¡fico mostrando errores por tipo de consulta]
[GrÃ¡fico de latencia vs nÃºmero de herramientas]

### 5. Dashboard Visual
[Captura completa del dashboard]
[DescripciÃ³n de cada panel]

### 6. Protocolos de Seguridad
[Diagrama de flujo de validaciÃ³n]
[Tabla de medidas implementadas]

### 7. Recomendaciones
[Lista de 5 recomendaciones con impacto]

### 8. Conclusiones
[Resumen de logros y prÃ³ximos pasos]

### 9. Referencias (APA)
- LangChain Documentation (2024)
- Streamlit Documentation (2024)
- OpenAI API Reference (2024)
- Plotly Python Graphing Library (2024)
- Python Logging Documentation (2024)
```

**Capturas obligatorias (mÃ­nimo 8):**
1. Dashboard completo
2. Panel de KPIs
3. GrÃ¡fico de latencia
4. MÃ©tricas de recursos
5. DistribuciÃ³n de errores
6. Herramientas mÃ¡s usadas
7. Ejemplo de logs JSON
8. Tabla de recomendaciones

**Impacto:** +5%

---

### **IE9: Lenguaje TÃ©cnico y ArgumentaciÃ³n (5%)**

ğŸ“Š **Nivel alcanzado:** âš ï¸ **Buen desempeÃ±o (80% = 4/5 puntos)**

**Estado actual:**
- âœ… CÃ³digo bien documentado
- âœ… Comentarios tÃ©cnicos apropiados
- âš ï¸ Falta informe con argumentaciÃ³n

**Lo que necesitas:**

En el informe, usar terminologÃ­a tÃ©cnica:
- âœ… Observabilidad, mÃ©tricas, KPIs
- âœ… Latencia, throughput, overhead
- âœ… Trazabilidad, logging, debugging
- âœ… AnomalÃ­as, patrones, outliers
- âœ… Escalabilidad, sostenibilidad

**Ejemplo de argumentaciÃ³n:**
```markdown
La implementaciÃ³n de cachÃ© de respuestas se fundamenta en el anÃ¡lisis 
de logs que revelÃ³ que el 40% de las consultas son repetidas (patrÃ³n 
identificado mediante anÃ¡lisis de embeddings con similitud >0.95). 
Esta optimizaciÃ³n reducirÃ­a la latencia promedio de 2.5s a 1.0s 
(mejora del 60%), disminuyendo el overhead de llamadas a la API de 
OpenAI y mejorando el throughput del sistema de 24 a 60 queries/minuto.
```

**Impacto:** +1% (llegarÃ­as a 5/5)

---

## 3. TABLA RESUMEN DE CUMPLIMIENTO

| Indicador | Peso | Actual | Nivel | Con Mejoras | Puntos Actuales | Puntos Posibles |
|-----------|------|--------|-------|-------------|-----------------|-----------------|
| IE1: MÃ©tricas PrecisiÃ³n | 15% | âŒ | 0% | âœ… 100% | 0 | 15 |
| IE2: MÃ©tricas Latencia | 15% | âš ï¸ | 60% | âœ… 100% | 9 | 15 |
| IE3: AnÃ¡lisis Logs | 15% | âš ï¸ | 80% | âœ… 100% | 12 | 15 |
| IE4: Patrones/AnomalÃ­as | 10% | âŒ | 0% | âœ… 100% | 0 | 10 |
| IE5: Dashboard Visual | 15% | âŒ | 0% | âœ… 100% | 0 | 15 |
| IE6: Seguridad | 10% | âš ï¸ | 60% | âœ… 100% | 6 | 10 |
| IE7: Recomendaciones | 10% | âŒ | 0% | âœ… 100% | 0 | 10 |
| IE8: Informe Visual | 5% | âŒ | 0% | âœ… 100% | 0 | 5 |
| IE9: Lenguaje TÃ©cnico | 5% | âš ï¸ | 80% | âœ… 100% | 4 | 5 |
| **TOTAL** | **100%** | | **31%** | **100%** | **31** | **100** |

**ğŸ“Š Nota proyectada actual:** 31/100 (2.2/7.0) âš ï¸ **REPROBADO**  
**ğŸ“Š Nota proyectada con mejoras:** 100/100 (7.0/7.0) âœ… **EXCELENTE**

---

## 4. PROBLEMAS CRÃTICOS

### ğŸš¨ CRÃTICO (afecta 45% de la nota):
1. **NO existe dashboard visual** â†’ PÃ©rdida de 15%
2. **NO hay mÃ©tricas de precisiÃ³n/consistencia** â†’ PÃ©rdida de 15%
3. **NO hay anÃ¡lisis de patrones** â†’ PÃ©rdida de 10%
4. **NO hay informe tÃ©cnico** â†’ PÃ©rdida de 5%

### âš ï¸ IMPORTANTE (afecta 25%):
5. **MÃ©tricas de recursos incompletas** â†’ PÃ©rdida de 6%
6. **Logs no estructurados (JSON)** â†’ PÃ©rdida de 3%
7. **Sin recomendaciones documentadas** â†’ PÃ©rdida de 10%
8. **Seguridad parcial** â†’ PÃ©rdida de 4%
9. **Sin validaciÃ³n de inputs** â†’ PÃ©rdida de 2%

### ğŸ’¡ MENOR (afecta 5%):
10. **Lenguaje tÃ©cnico mejorable en informe** â†’ PÃ©rdida de 1%

---

## 5. QUICK WINS (Mejoras RÃ¡pidas <4h)

| # | Mejora | Tiempo | Impacto | Archivos |
|---|--------|--------|---------|----------|
| 1 | Logs en formato JSON | 1h | +3% (IE3) | `logger.py` lÃ­nea 52 |
| 2 | ValidaciÃ³n de inputs | 2h | +2% (IE6) | Crear `src/security/validator.py` |
| 3 | MÃ©tricas de tokens | 1h | +2% (IE2) | `agent_executor.py` |
| 4 | Rate limiting bÃ¡sico | 2h | +2% (IE6) | Crear `src/security/rate_limiter.py` |
| 5 | Documentar seguridad | 1h | +2% (IE6) | Informe secciÃ³n 6 |

**Total Quick Wins:** 7 horas â†’ +11% en la nota

---

## 6. PLAN DE ACCIÃ“N PRIORIZADO

### **Semana 1 (20 horas):**

**Prioridad 1: Dashboard Visual** (8h) â†’ +15%
- Crear `dashboard.py` con Streamlit + Plotly
- 5 paneles: KPIs, Latencia, Recursos, Errores, Herramientas
- Capturar 8 screenshots

**Prioridad 2: MÃ©tricas de PrecisiÃ³n** (4h) â†’ +15%
- Crear `src/monitoring/metrics.py`
- Implementar: precision, consistency, error_frequency
- Integrar en `app_agent.py`

**Prioridad 3: AnÃ¡lisis de Patrones** (4h) â†’ +10%
- FunciÃ³n `identify_patterns()` en `metrics.py`
- Detectar: errores por tipo, latencia por herramientas, horarios pico
- Documentar 3 patrones + 3 anomalÃ­as

**Prioridad 4: Quick Wins** (4h) â†’ +11%
- Logs JSON
- ValidaciÃ³n inputs
- Rate limiting
- MÃ©tricas tokens

**Total Semana 1:** +51% â†’ Nota sube a 82/100 (5.7/7.0)

---

### **Semana 2 (12 horas):**

**Prioridad 5: Informe TÃ©cnico** (6h) â†’ +5%
- Crear `docs/INFORME_EP3.pdf` (5 pÃ¡ginas)
- Incluir 8 capturas del dashboard
- 5 referencias APA

**Prioridad 6: Recomendaciones** (3h) â†’ +10%
- Documentar 5 recomendaciones basadas en datos
- Formato: Problema â†’ Propuesta â†’ Impacto â†’ Prioridad

**Prioridad 7: Completar MÃ©tricas de Recursos** (2h) â†’ +6%
- Agregar: RAM, CPU, tokens
- Contextualizar: promedio, mÃ¡x, mÃ­n

**Prioridad 8: Pulir Lenguaje TÃ©cnico** (1h) â†’ +1%
- Revisar informe con terminologÃ­a apropiada
- Argumentar con datos

**Total Semana 2:** +22% â†’ Nota sube a 104/100 (7.0/7.0) âœ…

---

### **Semana 3 (8 horas):**

**RevisiÃ³n Final:**
- Testing completo del dashboard
- Verificar todas las capturas
- Revisar informe (ortografÃ­a, formato)
- Preparar presentaciÃ³n
- Backup del proyecto

---

## 7. ESTRUCTURA DE ARCHIVOS REQUERIDA

```
EVALUACION 1 SOLUCIONES IA/
â”œâ”€â”€ dashboard.py                    âš ï¸ CREAR (CRÃTICO)
â”œâ”€â”€ app_agent.py                    âœ… Existe (modificar)
â”œâ”€â”€ README.md                       âš ï¸ Actualizar
â”œâ”€â”€ requirements.txt                âš ï¸ Agregar: plotly, psutil
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ monitoring/                 âš ï¸ CREAR CARPETA
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ metrics.py              âš ï¸ CREAR (CRÃTICO)
â”‚   â”‚   â””â”€â”€ analyzer.py             âš ï¸ CREAR
â”‚   â”‚
â”‚   â”œâ”€â”€ security/                   âš ï¸ CREAR CARPETA
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ validator.py            âš ï¸ CREAR
â”‚   â”‚   â””â”€â”€ rate_limiter.py         âš ï¸ CREAR
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ logger.py               âœ… Existe (modificar a JSON)
â”‚   â”‚
â”‚   â””â”€â”€ [resto de archivos]         âœ… Mantener
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ INFORME_EP3.pdf             âš ï¸ CREAR (CRÃTICO)
â”‚   â”œâ”€â”€ screenshots/                âš ï¸ CREAR CARPETA
â”‚   â”‚   â”œâ”€â”€ dashboard_full.png
â”‚   â”‚   â”œâ”€â”€ kpis.png
â”‚   â”‚   â”œâ”€â”€ latency_chart.png
â”‚   â”‚   â”œâ”€â”€ resources.png
â”‚   â”‚   â”œâ”€â”€ errors.png
â”‚   â”‚   â”œâ”€â”€ tools.png
â”‚   â”‚   â”œâ”€â”€ logs_json.png
â”‚   â”‚   â””â”€â”€ recommendations.png
â”‚   â”‚
â”‚   â””â”€â”€ [archivos existentes]       âœ… Mantener
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ metrics/                    âš ï¸ CREAR CARPETA
â”‚       â”œâ”€â”€ metrics_history.json
â”‚       â””â”€â”€ patterns_analysis.json
â”‚
â””â”€â”€ logs/                           âœ… Existe
    â””â”€â”€ [archivos .log]             âœ… Cambiar a .json
```

---

## 8. CÃ“DIGO ESENCIAL A IMPLEMENTAR

### **1. dashboard.py** (CRÃTICO - 15%)

```python
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import json
from pathlib import Path

st.set_page_config(
    page_title="Dashboard Observabilidad",
    page_icon="ğŸ“Š",
    layout="wide"
)

# Cargar mÃ©tricas
def load_metrics():
    with open('data/metrics/metrics_history.json') as f:
        return json.load(f)

metrics = load_metrics()

# === TÃTULO ===
st.title("ğŸ“Š Dashboard de Observabilidad - Agente IA PastelerÃ­a 1000 Sabores")
st.markdown("---")

# === PANEL 1: KPIs ===
st.header("ğŸ“ˆ Indicadores Clave de Rendimiento (KPIs)")
col1, col2, col3, col4, col5 = st.columns(5)

with col1:
    st.metric(
        "Total Consultas",
        metrics['total_queries'],
        delta=f"+{metrics['queries_today']} hoy"
    )

with col2:
    st.metric(
        "PrecisiÃ³n",
        f"{metrics['precision']:.1f}%",
        delta=f"{metrics['precision_delta']:+.1f}%"
    )

with col3:
    st.metric(
        "Latencia Promedio",
        f"{metrics['avg_latency']:.0f}ms",
        delta=f"{metrics['latency_delta']:+.0f}ms",
        delta_color="inverse"
    )

with col4:
    st.metric(
        "Tasa de Error",
        f"{metrics['error_rate']:.1f}%",
        delta=f"{metrics['error_delta']:+.1f}%",
        delta_color="inverse"
    )

with col5:
    st.metric(
        "Tokens/Query",
        f"{metrics['avg_tokens']:.0f}",
        delta=f"{metrics['tokens_delta']:+.0f}"
    )

st.markdown("---")

# === PANEL 2: LATENCIA ===
col1, col2 = st.columns([2, 1])

with col1:
    st.subheader("â±ï¸ EvoluciÃ³n de Latencia")
    df_latency = pd.DataFrame(metrics['latency_history'])
    fig = px.line(
        df_latency,
        x='timestamp',
        y='latency_ms',
        title='Latencia en el Tiempo',
        labels={'latency_ms': 'Latencia (ms)', 'timestamp': 'Fecha/Hora'}
    )
    fig.add_hline(
        y=metrics['avg_latency'],
        line_dash="dash",
        annotation_text="Promedio"
    )
    st.plotly_chart(fig, use_container_width=True)

with col2:
    st.subheader("ğŸ“Š EstadÃ­sticas")
    st.metric("Promedio", f"{metrics['avg_latency']:.0f}ms")
    st.metric("MÃ¡ximo", f"{metrics['max_latency']:.0f}ms")
    st.metric("MÃ­nimo", f"{metrics['min_latency']:.0f}ms")
    st.metric("Desv. EstÃ¡ndar", f"{metrics['std_latency']:.0f}ms")

st.markdown("---")

# === PANEL 3: RECURSOS ===
st.subheader("ğŸ’» Uso de Recursos del Sistema")
col1, col2, col3 = st.columns(3)

with col1:
    fig = go.Figure(go.Indicator(
        mode="gauge+number+delta",
        value=metrics['current_memory_mb'],
        delta={'reference': metrics['avg_memory_mb']},
        title={'text': "Memoria RAM (MB)"},
        gauge={
            'axis': {'range': [None, 1000]},
            'bar': {'color': "darkblue"},
            'steps': [
                {'range': [0, 500], 'color': "lightgray"},
                {'range': [500, 750], 'color': "yellow"},
                {'range': [750, 1000], 'color': "red"}
            ],
            'threshold': {
                'line': {'color': "red", 'width': 4},
                'thickness': 0.75,
                'value': 800
            }
        }
    ))
    st.plotly_chart(fig, use_container_width=True)

with col2:
    fig = go.Figure(go.Indicator(
        mode="gauge+number+delta",
        value=metrics['current_cpu_percent'],
        delta={'reference': metrics['avg_cpu_percent']},
        title={'text': "CPU (%)"},
        gauge={
            'axis': {'range': [None, 100]},
            'bar': {'color': "darkgreen"},
            'steps': [
                {'range': [0, 50], 'color': "lightgray"},
                {'range': [50, 75], 'color': "yellow"},
                {'range': [75, 100], 'color': "red"}
            ]
        }
    ))
    st.plotly_chart(fig, use_container_width=True)

with col3:
    fig = go.Figure(go.Indicator(
        mode="gauge+number+delta",
        value=metrics['current_tokens'],
        delta={'reference': metrics['avg_tokens']},
        title={'text': "Tokens Consumidos"},
        gauge={
            'axis': {'range': [None, 4000]},
            'bar': {'color': "purple"}
        }
    ))
    st.plotly_chart(fig, use_container_width=True)

st.markdown("---")

# === PANEL 4: ERRORES ===
col1, col2 = st.columns(2)

with col1:
    st.subheader("âŒ DistribuciÃ³n de Errores")
    df_errors = pd.DataFrame(
        metrics['error_types'].items(),
        columns=['Tipo de Error', 'Frecuencia']
    )
    fig = px.bar(
        df_errors,
        x='Tipo de Error',
        y='Frecuencia',
        color='Frecuencia',
        color_continuous_scale='Reds'
    )
    st.plotly_chart(fig, use_container_width=True)

with col2:
    st.subheader("ğŸ“‰ Errores en el Tiempo")
    df_errors_time = pd.DataFrame(metrics['errors_history'])
    fig = px.line(
        df_errors_time,
        x='timestamp',
        y='error_count',
        title='Frecuencia de Errores'
    )
    st.plotly_chart(fig, use_container_width=True)

st.markdown("---")

# === PANEL 5: HERRAMIENTAS ===
col1, col2 = st.columns(2)

with col1:
    st.subheader("ğŸ”§ Herramientas MÃ¡s Usadas")
    df_tools = pd.DataFrame(
        metrics['tool_usage'].items(),
        columns=['Herramienta', 'Usos']
    )
    fig = px.pie(
        df_tools,
        values='Usos',
        names='Herramienta',
        hole=0.3
    )
    st.plotly_chart(fig, use_container_width=True)

with col2:
    st.subheader("â±ï¸ Latencia por Herramienta")
    df_tool_latency = pd.DataFrame(metrics['tool_latency'])
    fig = px.bar(
        df_tool_latency,
        x='tool',
        y='avg_latency_ms',
        color='avg_latency_ms',
        color_continuous_scale='Blues'
    )
    st.plotly_chart(fig, use_container_width=True)

st.markdown("---")

# === PANEL 6: PATRONES Y ANOMALÃAS ===
st.subheader("ğŸ” Patrones y AnomalÃ­as Detectadas")

tab1, tab2 = st.tabs(["Patrones", "AnomalÃ­as"])

with tab1:
    for pattern in metrics['patterns']:
        st.info(f"**{pattern['title']}**: {pattern['description']}")

with tab2:
    for anomaly in metrics['anomalies']:
        st.warning(f"**{anomaly['type']}**: {anomaly['description']}")
```

### **2. src/monitoring/metrics.py** (CRÃTICO - 15%)

```python
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List
import numpy as np

class ObservabilityMetrics:
    def __init__(self, data_dir='data/metrics'):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        self.total_queries = 0
        self.correct_responses = 0
        self.consistency_scores = []
        self.errors = []
        self.latencies = []
        self.resource_usage = []
        self.tool_usage = {}
    
    def record_query(
        self,
        query: str,
        response: str,
        is_correct: bool,
        latency_ms: float,
        memory_mb: float,
        cpu_percent: float,
        tokens: int,
        tools_used: List[str],
        error: str = None
    ):
        """Registra una query completa con todas sus mÃ©tricas"""
        self.total_queries += 1
        
        if is_correct:
            self.correct_responses += 1
        
        if error:
            self.errors.append({
                'timestamp': datetime.now().isoformat(),
                'error': error,
                'query': query
            })
        
        self.latencies.append({
            'timestamp': datetime.now().isoformat(),
            'latency_ms': latency_ms
        })
        
        self.resource_usage.append({
            'timestamp': datetime.now().isoformat(),
            'memory_mb': memory_mb,
            'cpu_percent': cpu_percent,
            'tokens': tokens
        })
        
        for tool in tools_used:
            self.tool_usage[tool] = self.tool_usage.get(tool, 0) + 1
    
    def calculate_precision(self) -> float:
        """PrecisiÃ³n = respuestas correctas / total"""
        if self.total_queries == 0:
            return 0.0
        return (self.correct_responses / self.total_queries) * 100
    
    def calculate_error_frequency(self) -> float:
        """Errores por cada 100 consultas"""
        if self.total_queries == 0:
            return 0.0
        return (len(self.errors) / self.total_queries) * 100
    
    def get_latency_stats(self) -> Dict:
        """EstadÃ­sticas de latencia"""
        if not self.latencies:
            return {}
        
        values = [l['latency_ms'] for l in self.latencies]
        return {
            'avg': np.mean(values),
            'max': np.max(values),
            'min': np.min(values),
            'std': np.std(values)
        }
    
    def export_metrics(self):
        """Exporta mÃ©tricas a JSON para el dashboard"""
        latency_stats = self.get_latency_stats()
        
        metrics = {
            'timestamp': datetime.now().isoformat(),
            'total_queries': self.total_queries,
            'precision': self.calculate_precision(),
            'error_rate': self.calculate_error_frequency(),
            'avg_latency': latency_stats.get('avg', 0),
            'max_latency': latency_stats.get('max', 0),
            'min_latency': latency_stats.get('min', 0),
            'std_latency': latency_stats.get('std', 0),
            'latency_history': self.latencies,
            'error_types': self._count_error_types(),
            'tool_usage': self.tool_usage,
            'resource_usage': self.resource_usage
        }
        
        output_file = self.data_dir / 'metrics_history.json'
        with open(output_file, 'w') as f:
            json.dump(metrics, f, indent=2)
        
        return metrics
    
    def _count_error_types(self) -> Dict:
        """Cuenta tipos de errores"""
        error_types = {}
        for error in self.errors:
            error_type = error['error'].split(':')[0]
            error_types[error_type] = error_types.get(error_type, 0) + 1
        return error_types
```

---

## 9. RECOMENDACIONES FINALES

### âœ… **Fortalezas del Proyecto:**
- Agente ReAct bien implementado (EP2)
- CÃ³digo modular y organizado
- Sistema de logging funcional
- Memoria dual operativa

### ğŸ¯ **Ãreas de Oportunidad:**
- **Observabilidad**: Implementar dashboard y mÃ©tricas completas
- **DocumentaciÃ³n**: Crear informe tÃ©cnico con evidencia visual
- **AnÃ¡lisis**: Identificar patrones y proponer mejoras basadas en datos

### ğŸ’¡ **Consejo Final:**
**Prioriza el dashboard (IE5 - 15%)** y **mÃ©tricas de precisiÃ³n (IE1 - 15%)**. Estos dos componentes te dan 30% de la nota y son los mÃ¡s visibles en la presentaciÃ³n. Con 12 horas de trabajo enfocado, puedes pasar de 31% a 61% (4.3/7.0 = APROBADO).

Luego, en la segunda semana, completa el informe y recomendaciones para llegar a 90-100%.

---

**ğŸ“Š NOTA FINAL PROYECTADA CON PLAN COMPLETO: 95-100/100 (6.7-7.0/7.0)**

Â¿Listo para empezar? Te recomiendo comenzar por el dashboard. Â¿Quieres que te ayude a implementarlo? ğŸš€
